Title: How Big Tech is exploiting existing copyright exceptions
Subtitle: and other developments of the week ended October 8th
Author: Peter Schoppert
URL: https://aicopyright.substack.com/p/how-big-tech-is-exploiting-existing
---
_A weekly email update on developments in the world of large language models (LLMs) and image (and just in the last two weeks, video) synthesizers, with a specific focus on the question of how the legal uncertainty around these models will be sorted. For more background, please read[here.](https://aicopyright.substack.com/about)_

Last week we mentioned Meta‚Äôs new Make-a-Video model that creates short videos based on text prompts. Such is the new awareness of the issues involved in these models that it only a took a few days for someone to start to dig into the sources on which this model was trained. According to [Andy Baio, ](https://waxy.org/about/)the model was trained on 10.7m videos scraped from stock imagery site Shutterstock (the WebVid-10M dataset), and 10m video clips copied by Microsoft Research Asia from Youtube. The videos on the Shutterstock website are for sale, but are publically availabile in lower resolution **and water-marked** versions. These were the versions that were scraped, watermarks and all. 

Thanks for reading AI and Copyright! Subscribe for free to receive new posts and support my work.

Subscribe

Baio read [the academic paper](https://www.arxiv-vanity.com/papers/2104.00650/) by the folks who compiled the first dataset to find that they were aware of the copyight implications, and did the copying under the UK data-mining exception, ‚ÄúThe use of data collected for this study is authorised via the Intellectual Property Office‚Äôs Exceptions to Copyright for Non-Commercial Research and Private Study.‚Äù

So the dataset is compiled by researchers, but then it is used by Big Tech to create the models that are transforming the whole field of creative work. As Baio headlined this week‚Äôs must-read:

#### [AI Data Laundering: How Academic and Nonprofit Researchers Shield Tech Companies from Accountability](https://waxy.org/2022/09/ai-data-laundering-how-academic-and-nonprofit-researchers-shield-tech-companies-from-accountability/)

Biao and associate [Simon Willinson](https://simonwillison.net/) had a week earlier published their findings from exploring the LAION data used to train image synthesizer Stable Diffusion, the subject of [last week‚Äôs newsletter](https://aicopyright.substack.com/p/algorithmic-disgorgement-isis-executions). Given the enormous size of this dataset, the tool explores only 2% of the image data. This small subset, not a random sample, shows that images came from a variety of domains hosting copyrighted material, including a million images from pinterest.com. Blog hosting sites were an important component of this, as were shopping sites, and stock image sites, including Shutterstock and Getty Images. The more complete analysis, including by artist, is [here.](https://waxy.org/2022/08/exploring-12-million-of-the-images-used-to-train-stable-diffusions-image-generator/)

‚ÄúIt‚Äôs become standard practice for technology companies working with AI to commercially use datasets and models collected and trained by non-commercial research entities like universities or non-profits. In some cases, they‚Äôre directly funding that research.‚Äù - Andy Biao

LAION‚Äôs computing time was sponsored by Stability AI, the company which then turned around and raised a rumoured $100m in venture capital on the basis of the success of the model.

 _The commercial/non-commercial distinction was always going to be difficult to define and defend. But it is clear that BigTech companies are actively pushing the data gathering work to third party non-profits to avoid responsibility._ _The_ _Common Crawl dataset seems the paragon of these efforts,[chaired by ex-Googler Gil Elbaz,](https://commoncrawl.org/about/team/) with well-known Silicon Valley figures like Carl Malamud_ _and Nova Spivack on the Board._

#### So I guess that explains why my digital art has watermarks‚Ä¶?

In an adjacent area of cyberspace, a well-known digital artist was wondering why the images she was generating were showing watermarks

[![Twitter avatar for @quasimondo](https://substackcdn.com/image/twitter_name/w_96/quasimondo.jpg)Mario Klingemann üá∫üá¶ @quasimondoDoes anyone know if there is a site that uses this type of watermarking? Or is this some artist's signature style? Just got a whole bunch of them and quite like the principle, but would not want to plagiarize out of ignorance. #stablediffusion ![Image](https://substackcdn.com/image/fetch/w_600,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFedRS7zXgAgPek9.png)](https://twitter.com/quasimondo/status/1578321412036534272?s=46&t=IwAupGOyGM0i__8bjPwWNQ)[9:48 AM ‚àô Oct 7, 2022

* * *

23Likes2Retweets](https://twitter.com/quasimondo/status/1578321412036534272?s=46&t=IwAupGOyGM0i__8bjPwWNQ)

#### [UK House of Lords hearing on the future of the creative industries to be addressed by a chatbot ‚Ä¶ this is being hyped as the first robot to address the House of Lords ](https://news.yahoo.com/robot-address-house-lords-first-105837913.html?guccounter=1)

It will evidently answer questions on whether AI is a threat to creative industries. 

_It is exactly this kind of stunt that is obscuring the real debate on these matters. Leave it to the Lords to fall for this sort of thing‚Ä¶_

#### If they don‚Äôt want to talk to a bot named Ai-Da, the Lords could chat with Musk, Xi, Trump or Oliver Wendell Holmes

The Washington Post has [a useful piece](https://www.washingtonpost.com/technology/2022/10/07/characterai-google-lamda/) on the launch of Character.ai, a ‚Äúcreate your own chatbot‚Äù service launched by former Googlers. The chatbots are built on LLMs and can be trained to take on certain voices or tasks. In the wee jetlagged hours I created an Oliver Wendell Holmes Jr chatbot [if you‚Äôd like to meet him](https://beta.character.ai/chat?char=Dohb_7O_a6DnD35j-X2nAGKyP1049K7XQoDkGCx9QD8) (though you will need to register, etc.)

The piece quotes Timnit Gebru, and this describes well the experience of putting together this newsletter over the last few weeks:

> The speed with which industry fascination has swerved from language models to text-to-3D video is alarming when trust and safety advocates are still grappling with harms on social media, Gebru said. ‚ÄúWe‚Äôre talking about making horse carriages safe and regulating them and they‚Äôve already created cars and put them on the roads,‚Äù she said.

####  _And in other news‚Ä¶_

#### [Springer Nature announces that it is trialing AI-powered editing tools](https://group.springernature.com/gp/group/media/press-releases/expands-ai-driven-digital-editing-services-for-books/23309226)

 _Well so is NUS Press, but we didn‚Äôt make a big noise about it! ;-) The narrative from Springer Nature continues to reinforce the conceptualisation/instantiation divide, the idea that coming up with the big ideas is the important and creative thing, the rest is grunt work. According to the press release, the new tool will allow authors_ ‚Äúto spend less time preparing their work for publication and have more time doing the research that drives society forward.‚Äù _In the humanities, ‚Äúpreparing the work for publication‚Äù_ is _the research._

#### [Article draws attention to the use of image synthesis to enable fraud in scientific publication. ](https://www.cell.com/patterns/fulltext/S2666-3899\(22\)00103-9)

> ‚Äúsuch advanced generative models threaten the publishing system in academia as they may be used to generate fake scientific images that cannot be effectively identified. We demonstrate the disturbing potential of these generative models in synthesizing fake images, plagiarizing existing images, and deliberately modifying images. It is very difficult to identify images generated by these models by visual inspection, image-forensic tools, and detection tools due to the unique paradigm of the generative models for processing images.‚Äù

Thanks for reading AI and Copyright! Subscribe for free to receive new posts.

Subscribe
