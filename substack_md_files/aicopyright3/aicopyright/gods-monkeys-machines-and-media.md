Title: Gods, monkeys, machines and media
Subtitle: USCO rules on the output question, stirrings from the media industry and how OpenAI is really doing us a big favour
Author: Peter Schoppert
URL: https://aicopyright.substack.com/p/gods-monkeys-machines-and-media
---
### Stirrings

First, some stirrings from the media industry. Warner Bros and News Corp [go on the record](https://www.bloomberg.com/news/articles/2023-02-17/openai-is-faulted-by-media-for-using-articles-to-train-chatgpt) to Bloomberg that they are looking carefully at OpenAI’s use of their content to train Chat-GPT. If Warner Bros and News Corp open their eyes a bit wider they will find that their content has been consistently scraped all this while _**by everyone training a large language model since 2019**_. Publisher content is a key element in large language models, including those from Alphabet, Facebook and other Big Tech companies. 

> “Anyone who wants to use the work of Wall Street Journal journalists to train artificial intelligence should be properly licensing the rights to do so from Dow Jones,” Jason Conti, general counsel for News Corp.’s Dow Jones unit, said in a statement provided to Bloomberg News. “Dow Jones does not have such a deal with OpenAI.”
> 
> Conti added: “We take the misuse of our journalists’ work seriously, and are reviewing this situation.”
> 
> Like the Journal, CNN believes that using its articles to train ChatGPT violates the network’s terms of service, according to a person with knowledge of the matter. The network, owned by Warner Bros. Discovery Inc., plans to reach out to OpenAI about being paid to license the content, said the person, who asked not to be identified discussing a legal matter.

### For fans of the output question, much to chew on from USCO

The USCO has decided to amend and reissue the copyright registration previously granted to Kristina Kashtanova for her Midjourney-assisted graphic novel _Zarya of the Dawn._

They allowed that she has copyright in the text of her graphic novel, and a compilation right in the arrangement of text and images. But the office concludes that “the images in the Work that were generated by the Midjourney technology are not the product of human authorship.”

The decision is contained [in a letter from Robert J. Kasunic, Associate Register of Copyrights](https://drive.google.com/file/d/1EK7jKVqRz_GP0kJPzOZY_HqKW0n2ML6c/view). The precedents that non-human agents are not given copyright are set out quite clearly, including mention of Urantia Found. v. Kristen Maaherra, in which the courts refused to grant copyright to “non-human spiritual beings”, on the basis that “it is not creations of divine beings that the copyright laws were intended to protect.” That made me smile. The famed selfie-taking crested black macaque also gets a mention.

Subscribe

The key section looks at whether Kashtanova’s images can be copyrighted, given that they were created by/using Midjourney. This discussion starts off with the question of “How Midjourney Works”. So we see how crucial the “metaphors” are to the legal thinking. I won’t critique the characterisation made (still saving my fire on this question for diffusion models), but the metaphor used by Kasunic comes out rather clearly in the next section, D.2 “Application of Copyright Law to Midjourney Images”.

Here is the key para:

> Because Midjourney starts with randomly generated noise that evolves into a final image, there is no guarantee that a particular prompt will generate any particular visual output. Instead, prompts function closer to suggestions than orders, similar to the situation of a client who hires an artist to create an image with general directions as to its contents. If Ms. Kashtanova had commissioned a visual artist to produce an image containing “a holographic elderly white woman named Raya,” where “[R]aya is having curly hair and she is inside a spaceship,” with directions that the image have a similar mood or style to a “Star Trek spaceship,” “a hologram,” an “octane render,” “unreal engine,” and be “cinematic” and “hyper detailed,” Ms. Kashtanova would not be the author of that image.

If you stick with that metaphor, then you would have to argue that Midjourney _**does**_ create works on its own, and the only reason they cannot be copyrighted is because copyright is not intended to protect the creations of machines, monkeys or gods. 

This is why I think focusing on the output question without considering the input question is a mistake. USCO seems to be saying that generative AI is not their problem. _I hope I’m wrong about that._ We still need to consider the origins of the machine-generated works in the training data, and how much they can be understood to be subsidiary to the copyrighted works they have been trained on. 

Kanusic’s letter also concludes that although Kashtanova spent a lot of energy tweaking Midjourney to get the output she desired, this was mere “sweat of the brow”, and not “the minimum creative spark” required for copyright in the US.

Well. Clearly put. But I have find myself agreeing rather more with the [perspective of Kashtanova’s lawyer, Van Lindberg,](https://www.processmechanics.com/2023/02/22/a-mixed-decision-from-the-us-copyright-office/) that the Copyright Office decision focuses too much on the random element inherent in diffusion models, and not enough on the creativity involved.

> “The standard is whether there is a "modicum of creativity," not whether Kris could "predict what Midjourney [would] create ahead of time." In other words, the Office incorrectly focused on the _output of the tool_ rather than the _input from the human_.
> 
> Jackson Pollack famously couldn't predict how the paint he used would drip onto the canvas. Pollack designed his paintings - he knew what he wanted the end result to be - but he used a process involving random dripping and flicking of paint to make his art. In music, each performance of John Cage's 4'33" is _entirely defined_ by the random sounds that are made by the audience and the world around the stage.”

There’s no contradiction between creativity and an element of randomness. Mozart created a “compose your own minuet” dice game back in 1787. There was certaintly [“computational art before computers”](https://www.pinterest.com/pin/443252788330099151/). Generating AI content definitely feels like creative work…

### Other points to watch on the regulatory front:

In the oral arguments to Gonzalez v Google, Supreme Court Justice Neil Gorsuch stated that the in/famous section 230 of the Communications Decency Act of 1996 would not apply to the output of generative AI. “Artificial intelligence generates poetry," [Gorsuch said during the hearings.](https://www.washingtonexaminer.com/policy/courts/gorsuch-chatgpt-section-230) “It generates polemics today that would be content that goes beyond picking, choosing, analyzing, or digesting content. And that is not protected.” Should this reasoning hold, it opens up a new area of liability for Google, Microsoft, OpenAI et al. 

The US Federal Trade Commission gets ambitious, and [announces a new Office of Technology](https://www.ftc.gov/news-events/news/press-releases/2023/02/ftc-launches-new-office-technology-bolster-agencys-work) with a specific focus on AI. 

> The [FTC’s] Office of Technology’s top priority is to work with staff and leadership across the agency to strengthen and support the agency on enforcement investigations and litigated cases. This could mean dissecting claims made about an AI-powered product to assess whether the offering is oozing with snake oil, or whether automated decision systems for teacher evaluations adversely impact employment decisions and make inferences that impact compensation and tenure.

And it’s nice to see the legal uncertainty foregrounded again in the press, as in a recent article article from Axios, [Generative AI is a legal minefield,](https://www.axios.com/2023/02/24/chatgpt-generative-ai-legal-minefield) that cites the input question (copyright and terms of service, quoting Butterick), the output question, and questions of liability in cases of slander, libel, and liability for AIs giving false information. 

Of course there has been so much other news in generative AI space, including now a debate about the political leanings of the AIs (or rather, of the Reinforcement Learning with Human Feedback models which attempt to “tame” them). But for the purposes of our newsletter, let me just highlight two recent postings.

### OpenAI guidelines

OpenAI is increasingly trying to shape the discussions around regulation, calling actively now for “society” to set the terms within which they operate. They are still very much presenting things from the perspective that “AGI is coming”, and that all this pain is justified because it is going to be so much worse/better when AGI is _really_ powerful. Sam Altman explains that he is really doing us a favour by introducing these products (that might be illegal and which have so much potential for harm):

> First, as we create successively more powerful systems, we want to deploy them and gain experience with operating them in the real world. We believe this is the best way to carefully steward AGI into existence—a gradual transition to a world with AGI is better than a sudden one. We expect powerful AI to make the rate of progress in the world much faster, and we think it’s better to adjust to this incrementally.

There is much else that’s weirdly fascinating in the post. Altman admits that job displacement is an impact of these new models (file that for your fourth factor arguments), and announces that OpenAI intends to manage the politics of the chatbot questions now coming to the fore by making “it easy for users to change the behavior of the AI they’re using”. _**What could possibly go wrong with that?**_

[Share](https://aicopyright.substack.com/p/gods-monkeys-machines-and-media?utm_source=substack&utm_medium=email&utm_content=share&action=share)

### Can submarines swim?

And next to last, you might enjoy this excellent GPT explainer - [Can Submarines Swim?,](https://open.substack.com/pub/rootsofprogress/p/can-submarines-swim) by entrepreneur and writer Jason Crawford. Not only does it usefully explain the way that the GPT operates as a statistical model, it makes the additional point which sometimes gets lost when one is emphasing this aspect of the models’ nature:

> There are two mistakes you can make in thinking about the future of AI. One is to assume that its processes are essentially no different from human thought. The other is to assume that if they are different, then an AI can’t do things that we consider to be very human.

Keeping both points in view is what makes the discourse so tough, and yet so fascinating.

### Keeping track of the cases

Given the “output question” nature of this week’s newsletter, I’m adding another case to our listing, Thaler v. Perlmutter et al, filed June 22, 2022. This is a challenge to USCO’s decision not to grant copyright to an AI-created work, with by Ryan Abbott acting for the plaintiffs.

  1. Butterick’s first class action against Microsoft/Github/OpenAI for Copilot

  2. [A second action filed by Butterick et al against Microsoft/Github/OpenAI](https://www.documentcloud.org/documents/23589446-microsoft-openai-complaint?responsive=1&title=1) \- Microsoft has [filed to dismiss,](http://ttps://www.theverge.com/2023/1/28/23575919/microsoft-openai-github-dismiss-copilot-ai-copyright-lawsuit) hearing in May 2023

  3. The class action against StabilityA/Midjourney/DeviantArt for Stable Diffusion

  4. Getty Images civil suit in the UK - not yet filed?

  5. [Getty Images civil suit in Delaware against Stability](https://www.scribd.com/document/624165761/Getty-Lawsuit-Against-Stable-Diffusion) \- filed Feb 4th, 2023

  6. [Thaler vs Perlmutter et al,](https://www.courtlistener.com/docket/63356475/thaler-v-perlmutter/) challenging USCO’s decision not to grant copyright to an AI, filed June 22, 2022.



